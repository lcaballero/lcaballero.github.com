<div class="writing">
	<div class="header">
		<h2>
			Web App - Performance Tuning</h2>
		<dl>
			<dt>Web App - Performance Tuning</dt>
			<dd>
				Lucas Caballero</dd>
			<dd>
				03-03-2012</dd>
			<dd>
				(Work in progress)</dd>
		</dl>
	</div>
	<div class="rail">
		<div class="tags">
			<h3>
				Tags</h3>
			<ul class="contain">
				<li>web-application</li>
				<li>performance-tuning</li>
				<li>page-speed</li>
			</ul>
		</div>
		<div class="toc">
			<h3>
				Table of Contents</h3>
			<ul class="contain">
				<li>Background</li>
				<li>Idea: Increase the slow-start payload</li>
				<li>Idea: Make sure Gzip is on</li>
			</ul>
		</div>
	</div>
	<div class="subsections">
		<div>
			<h3>
				Background</h3>
			<p>
				I was tasked with performance tuning a site again. This time I approached the task
				a bit differently because in this case I helped design the majority of the site
				structure/architecture, and I wanted the site to hum like some kind of Fararri.
				Not every idea I had is yet implemented and some of the ideas I have probably won't
				get done. Either because I don't have the time to do them myself, or because they
				are just beyond what anyone else has tried (at the firm I work at). Some folks feel
				the site is already 'fast-enough'.
			</p>
			<p>
				I'm writing this to reflect on some of the ideas I'm working with, and for those
				out there who have faced the problem too, and for whom it's not a matter of rearranging
				a site in some kind of panick because someone load-tested and found they could only
				get 6 request/sec, and now everyone is scrambling to improve throughput. For the
				folks in the latter circumstance, I feel for you, because in general it's a lack
				of forethought that tends to cause that problem, and a lack of forethought probably
				also means you have to dive into a muddled mess of speghetti code.
			</p>
		</div>
		<div>
			<h3>
				Idea: Increasing the slow-start payload</h3>
			<p>
				Avoiding some of the deep technical issues with this idea, this is mostly increasing
				the payload between network acknowledgements. Apparently, a number of folks are
				already doing this (namely Google and Microsoft). From what I've read, the TCP standard
				uses a measurement called 4W which is the beginning payload amount at the start
				of a TCP request. After the request is acknowledge (ack'ed) the server throttles
				up and delivers more, the server here is assuming that the client and the band-width
				between server and client can handle a little more "pressure" so to speak. This
				happens more an more eventually maxing out at some level that both the server and
				the client can handle.
			</p>
			<p>
				However, what this causes is additional 'acks' (acknowledgements) between client
				and server that causes chatter over the network. It also causes additional round
				trips between the two endpoints. Removing the chatter and the delay increases the
				performance of a page and potentially the network (reduced chatter).
			</p>
			<p>
				Unfortunately, from what I found IIS can't or doesn't have a feature directly built
				into it that can adjust this 'slow-start' size, it's at a lower level (TCP), I suppose.
				This isn't really my area of expertise. (Newer versions of IIS probably have this
				feature).
			</p>
			<ul class="contain references">
				<li><a href="http://samsaffron.com/archive/2012/03/01/why-upgrading-your-linux-kernel-will-make-your-customers-much-happier">
					Why upgrading your Linux Kernel will make your customers much happier</a> </li>
				<li><a href="http://blog.benstrong.com/2010/11/google-and-microsoft-cheat-on-slow.html">
					Google and Microsoft Cheat on Slow-Start. Should You? </a></li>
			</ul>
		</div>
		<div>
			<h3>
				Idea: Make sure Gzip is on</h3>
			<p>
				Most people right now will be rolling their eyes and thinking "no-shit". But let's
				be clear, this is not 'use gzip', this is make sure gzip is on. That is if you 'assume'
				it's on you will probably regret making that assumption. As it turns out, I was looking
				around for ways to answer the question: once the request is delivered to the
				browser what keeps it from rendering the page lickety-split?
			</p>
			<p>
				I installed <a href="http://code.google.com/speed/page-speed/docs/using_chrome.html">
					Google Page Speed (Chrome Extension).</a> -- it was just released and, it spotted
				the problem for me. Actually, I was testing on my local machine, and everything
				worked as expected, gzip on, etc. Then when I promoted and tested in acceptance
				it complained that I didn't have gzip on -- but oddly the response header declared
				Gzip 1. So, lesson learned, I can write a test to verify some performance tuning
				assumptions: Assert.True(IsGzipOn).
			</p>
			<p>
				Of course, <a href="http://code.google.com/speed/page-speed/docs/using_chrome.html">
					Google Page Speed</a> provides a ton of information on how to improve the in
				browser page performance, like minifying scripts and styles, or delaying the evaluation
				of long running scripts, which lead to some other ideas.
			</p>
		</div>
	</div>
</div>

